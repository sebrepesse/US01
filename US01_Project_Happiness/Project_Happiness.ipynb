{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datasets_ML = pd.read_pickle(\"savedDF_datasets_ML.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_datasets_ML[['restaurant_scale','Taxi_scale','mobike_scale']]\n",
    "Y = df_datasets_ML['happiness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = pd.DataFrame.copy(Y)\n",
    "Z[Y <= np.median(Y)] = \"Unhappy\"\n",
    "Z[Y > np.median(Y)] = \"Happy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Z_train, Z_test = train_test_split(X, Z, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=5, random_state=None, test_size=None, train_size=None),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort=False, random_state=None,\n",
       "                                              splitter='best'),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'max_depth': [1, 2, 4, 6, 8, 10, 20, 40, 100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to determine the best CV\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "param_grid = {'max_depth': [1,2,4,6,8,10,20,40,100]}\n",
    "search = GridSearchCV(tree.DecisionTreeClassifier(), param_grid, cv= ShuffleSplit(n_splits=5))\n",
    "search.fit(X,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = tree.DecisionTreeClassifier(max_depth=search.best_params_['max_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.775"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(max_depth=search.best_params_['max_depth'])\n",
    "clf = clf.fit(X_train, Z_train)\n",
    "prediction = clf.predict(X_test)\n",
    "np.mean(prediction == Z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(581.25, 641.6666666666666, 'X[0] <= -0.085\\nentropy = 0.497\\nsamples = 186\\nvalue = [86, 100]'),\n",
       " Text(290.625, 385.0, 'X[1] <= -0.474\\nentropy = 0.354\\nsamples = 109\\nvalue = [25, 84]'),\n",
       " Text(145.3125, 128.33333333333326, 'entropy = 0.221\\nsamples = 79\\nvalue = [10, 69]'),\n",
       " Text(435.9375, 128.33333333333326, 'entropy = 0.5\\nsamples = 30\\nvalue = [15, 15]'),\n",
       " Text(871.875, 385.0, 'X[0] <= 0.708\\nentropy = 0.329\\nsamples = 77\\nvalue = [61, 16]'),\n",
       " Text(726.5625, 128.33333333333326, 'entropy = 0.231\\nsamples = 45\\nvalue = [39, 6]'),\n",
       " Text(1017.1875, 128.33333333333326, 'entropy = 0.43\\nsamples = 32\\nvalue = [22, 10]')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,10))\n",
    "tree.plot_tree(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebr\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'n_estimators': [10, 40, 80, 90]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "param_grid={'n_estimators':[10,40,80,90]}\n",
    "clf=RandomForestClassifier()\n",
    "search=GridSearchCV(clf,param_grid,cv=5)\n",
    "search.fit(X_train,Z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=40,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=RandomForestClassifier(n_estimators=40)\n",
    "clf.fit(X_train,Z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.725"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Z_pred == Z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>Happy</th>\n",
       "      <th>Unhappy</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Happy</th>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unhappy</th>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>42</td>\n",
       "      <td>38</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  Happy  Unhappy  All\n",
       "True                          \n",
       "Happy         30       10   40\n",
       "Unhappy       12       28   40\n",
       "All           42       38   80"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "pd.crosstab(Z_test, Z_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebr\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                                          base_estimator=None,\n",
       "                                          learning_rate=1.0, n_estimators=50,\n",
       "                                          random_state=None),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'n_estimators': [10, 50, 100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We  want to determine the best param\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "param_grid = {'n_estimators': [10, 50, 100]}\n",
    "clf = AdaBoostClassifier()\n",
    "search = GridSearchCV(clf, param_grid, cv= 5)\n",
    "search.fit(X_train, Z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7125"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best model\n",
    "clf = AdaBoostClassifier(n_estimators=50)\n",
    "clf = clf.fit(X_train, Z_train)\n",
    "prediction = clf.predict(X_test)\n",
    "np.mean(prediction == Z_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Solution</th>\n",
       "      <th>clean</th>\n",
       "      <th>noise</th>\n",
       "      <th>smell</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Random Forrest</th>\n",
       "      <th>Adaboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>restaurant</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bicycle_part</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.5250</td>\n",
       "      <td>0.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bus</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>railway_station</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.6125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>convenience store</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.6750</td>\n",
       "      <td>0.6750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Solution  clean  noise  smell  Decision Tree  Random Forrest  \\\n",
       "0         restaurant     15      1      2         0.7250          0.6625   \n",
       "1       Bicycle_part     15      1      2         0.5875          0.5250   \n",
       "2                bus     15      1      2         0.6875          0.6875   \n",
       "3    railway_station     15      1      2         0.6250          0.6250   \n",
       "4  convenience store     15      1      2         0.7250          0.6750   \n",
       "\n",
       "   Adaboost  \n",
       "0    0.7125  \n",
       "1    0.5500  \n",
       "2    0.6875  \n",
       "3    0.6125  \n",
       "4    0.6750  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ML_results = pd.read_excel (\"results.xlsx\")\n",
    "df_ML_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ML_results.to_pickle(\"savedDF_results_ML.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, the final goal of my model will be to give recommandation to the City of Shanghai in order to improve the wellbeing of people. So, as I said before I took into consideration several parameters that we had to estimate the wellbeing such as the number of restaurant, bus, taxi or again green spaces. \n",
    "\n",
    "The first step of the project was to find a way to quantify this number of POI at Shanghai. This is the reason why I decided to use the dataset of the city of Shanghai that gave us all the neighborhoods of the city. This dataset countained all the geometry of each district and at the meantime the other datasets of parameters were using not geometry object but x and y location. I chose to convert these locations into a point geometry that I'll be able to use with the geometry set of the neighborhoods. Then, I count the number of point inside each polygon and now my datasets are ready for the Machine Learning part.\n",
    "\n",
    "For the machine learning part, I decided to use three different types of models: Decision Tree, Random Forrest and Adaboost. Each models will be as Classification because I don't want to predict numeric but here I want to predict different types of happiness (unhappy and happy only 2 because I have a small amount of data). \n",
    "\n",
    "The reason why I chose these 3 kind of models is that firstly, I want to have a model that is quick and explainable; it means that I prefer to have the result fast and that is easy to explain and understand. The Decision Tree is suitable for that. On top of that I wanted to make a comparison between this simpler model with 2 others models that is now not focus on a fast calculation but focus on the accuracy. I chose to study the Random Forrest and Adaboost for that. Moreover, each models will be solve using the cross validation tool that allow us better results.\n",
    "\n",
    "In the next part, I'll try to predict the wellbeing of shanghainese using different inputs : restaurant, Bicycle park, Bus, Railway entrance and exit, Convenience store, Scenic spot, Sport, Taxi, Green spaces, Mobike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will focus our discussion on the result provided inside the pickle above. First of all, during this part I just take into account one parameter each time. Then we will saw the results for multiple parameters at the same time. \n",
    "So, I wanted to highlight what is the solution of improving the wellbeing at Shanghai. That's why I decided to test my model with different coefficient on clean, smell and noise. For example, if I put a coefficient equal to 15 on clean then smell and noise have a coefficient of 1 it means that the people will be more sensitive of the clean parameter and we will be able to see which parameter can influence the most on the clean. Then if the City of Shanghai has to work on the clean they will be able to know on which point they have to focus their work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Clean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with the clean so we put a coefficient higher on this. Then we can observe that the maximum f1 score obtain is 73.75% with taxi's parameter. If we take the 10% higher values we can see that the restaurant, convenience store, taxi and mobike datas have the best f1 score. We can conclude that if people give more importance to the clean of a city then the city of Shanghai will have to focus their mind on these parameters. For example, we can imagine that if we have many restaurant or taxi the neighborhood will become more dirty because of the restaurant's trashes or the pollution of the taxi so the people won't be happy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Noise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same principle for the noise so we put the highest coefficient on it. We observe that the 10% higher values are from restaurant, bus, sport and mobike. So it means that the city of Shanghai has to for example reduce the number of bus in order to improve the satisfaction of the noise for people or again for instance add more mobikes because bikes are not noisy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Smell "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the smell we can see that the highest value are from restaurant and massively from mobikes. So the best solution to improve this parameter of smell will be to add more mobikes inside Shanghai in order to have less smell and people will be happier. It's easy to understand because bus and taxi bring bad smell that bikes don't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Same coefficient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If now we put the same coefficient on the three parameters clean, noise and smell we can see that the main thing that will give better results will be green spaces and sports. So if somewhere, all the parameter have a bad score it could be interesting for the city of Shanghai to add more green spaces and more place to do sports to improve the happiness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I decided to do calculations on the other combination: for example put the highest coefficient on clean and noise and the lowest coefficient on smell. I did 3 different combinations and I can now conclude on the results. \n",
    "The 10% highest score are obtained by restaurant, mobike and taxi. We can then conclude that the parameters that can influence the most on clean, noise and smell are these 3 parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Combinaison of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part that I want to deal with is the combinaison of multiples parameters at the same time for the machine learning. \n",
    "As we saw before, some of the parameters allow us to obtain a better f1 score depending the coefficient. We will now try to combine them in order to see if we will be able to have an even better f1 score. As we can see on the results file we tried some combinaison but we can observe that we don't have a real improvement of the f1 score. However we can clearly see that the best combinaison among these is to take into account the restaurant and mobike. Thanks to it, we can have a f1 score of 81.25% with a decision tree model. That's why, we can now conclude that if the city of Shanghai want to improve the wellbeing of shanghainese the main parameters to focus on are the restaurant and mobike ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
